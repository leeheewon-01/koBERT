{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e4cc85",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (8.0.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (4.0.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (6.15.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.5)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.30)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (63.4.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.11.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (304)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to c:\\users\\pc\\appdata\\local\\temp\\pip-req-build-zirxz7gc\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: boto3<=1.15.18 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from kobert==0.2.3) (1.15.18)\n",
      "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from kobert==0.2.3) (1.7.0.post2)\n",
      "Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from kobert==0.2.3) (1.8.0)\n",
      "Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Collecting torch<=1.10.1,>=1.7.0\n",
      "  Using cached torch-1.10.1-cp37-cp37m-win_amd64.whl (226.6 MB)\n",
      "Requirement already satisfied: transformers<=4.8.1,>=4.8.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from kobert==0.2.3) (4.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (2.0.7)\n",
      "Requirement already satisfied: protobuf in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (4.21.5)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from boto3<=1.15.18->kobert==0.2.3) (1.18.18)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.3.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: cython in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.32)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.18.4)\n",
      "Collecting numpy>=1.16.6\n",
      "  Using cached numpy-1.16.6-cp37-cp37m-win_amd64.whl (11.9 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.3.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.53)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.0)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch<=1.10.1,>=1.7.0\n",
      "  Using cached torch-1.10.0-cp37-cp37m-win_amd64.whl (226.6 MB)\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentencepiece<=0.1.96,>=0.1.6\n",
      "  Using cached sentencepiece-0.1.96-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "  Using cached mxnet-1.7.0.post1-py2.py3-none-win_amd64.whl (33.0 MB)\n",
      "  Using cached mxnet-1.6.0-py2.py3-none-win_amd64.whl (26.9 MB)\n",
      "  Using cached mxnet-1.5.0-py2.py3-none-win_amd64.whl (23.5 MB)\n",
      "  Using cached mxnet-1.4.1-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0.post0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gluonnlp<=0.10.0,>=0.6.0\n",
      "  Using cached gluonnlp-0.10.0-cp37-cp37m-win_amd64.whl\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.9.2.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.9.0.post0.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.3.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.2.tar.gz (237 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.1.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.8.0.tar.gz (235 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.7.1.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.7.0.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.6.0.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting boto3<=1.15.18\n",
      "  Using cached boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached boto3-1.15.17-py2.py3-none-any.whl (129 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of onnxruntime to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting onnxruntime<=1.8.0,==1.8.0\n",
      "  Using cached onnxruntime-1.8.0-cp37-cp37m-win_amd64.whl (4.7 MB)\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of kobert to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0.post0 depends on numpy<1.15.0 and >=1.8.2\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0 depends on numpy<1.15.0 and >=1.8.2\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' 'C:\\Users\\PC\\AppData\\Local\\Temp\\pip-req-build-zirxz7gc'\n",
      "ERROR: Cannot install kobert because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e19175",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\VSC\\bert\\KoBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'KoBERT' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3<=1.15.18 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from -r requirements.txt (line 1)) (1.15.18)\n",
      "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from -r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from -r requirements.txt (line 3)) (1.7.0.post2)\n",
      "Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from -r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from -r requirements.txt (line 5)) (0.1.96)\n",
      "Collecting torch<=1.10.1,>=1.7.0\n",
      "  Using cached torch-1.10.1-cp37-cp37m-win_amd64.whl (226.6 MB)\n",
      "Requirement already satisfied: transformers<=4.8.1,>=4.8.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from -r requirements.txt (line 7)) (4.8.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from onnxruntime<=1.8.0,==1.8.0->-r requirements.txt (line 4)) (4.21.5)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from onnxruntime<=1.8.0,==1.8.0->-r requirements.txt (line 4)) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from onnxruntime<=1.8.0,==1.8.0->-r requirements.txt (line 4)) (1.21.6)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from boto3<=1.15.18->-r requirements.txt (line 1)) (0.3.7)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from boto3<=1.15.18->-r requirements.txt (line 1)) (1.18.18)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from boto3<=1.15.18->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from gluonnlp<=0.10.0,>=0.6.0->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: cython in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from gluonnlp<=0.10.0,>=0.6.0->-r requirements.txt (line 2)) (0.29.32)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from mxnet<=1.7.0.post2,>=1.4.0->-r requirements.txt (line 3)) (2.18.4)\n",
      "Collecting numpy>=1.16.6\n",
      "  Using cached numpy-1.16.6-cp37-cp37m-win_amd64.whl (11.9 MB)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from mxnet<=1.7.0.post2,>=1.4.0->-r requirements.txt (line 3)) (0.8.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from torch<=1.10.1,>=1.7.0->-r requirements.txt (line 6)) (4.3.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from transformers<=4.8.1,>=4.8.1->-r requirements.txt (line 7)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from transformers<=4.8.1,>=4.8.1->-r requirements.txt (line 7)) (2022.8.17)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from transformers<=4.8.1,>=4.8.1->-r requirements.txt (line 7)) (0.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from transformers<=4.8.1,>=4.8.1->-r requirements.txt (line 7)) (0.10.3)\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch<=1.10.1,>=1.7.0\n",
      "  Using cached torch-1.10.0-cp37-cp37m-win_amd64.whl (226.6 MB)\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentencepiece<=0.1.96,>=0.1.6\n",
      "  Using cached sentencepiece-0.1.96-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "  Using cached mxnet-1.7.0.post1-py2.py3-none-win_amd64.whl (33.0 MB)\n",
      "  Using cached mxnet-1.6.0-py2.py3-none-win_amd64.whl (26.9 MB)\n",
      "  Using cached mxnet-1.5.0-py2.py3-none-win_amd64.whl (23.5 MB)\n",
      "  Using cached mxnet-1.4.1-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0.post0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gluonnlp<=0.10.0,>=0.6.0\n",
      "  Using cached gluonnlp-0.10.0-cp37-cp37m-win_amd64.whl\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.9.2.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.9.0.post0.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.3.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.2.tar.gz (237 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.1.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.8.0.tar.gz (235 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.7.1.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.7.0.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.6.0.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting boto3<=1.15.18\n",
      "  Using cached boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached boto3-1.15.17-py2.py3-none-any.whl (129 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of onnxruntime to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting onnxruntime<=1.8.0,==1.8.0\n",
      "  Using cached onnxruntime-1.8.0-cp37-cp37m-win_amd64.whl (4.7 MB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0.post0 depends on numpy<1.15.0 and >=1.8.2\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0 depends on numpy<1.15.0 and >=1.8.2\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install -r requirements.txt (line 2), -r requirements.txt (line 3) and -r requirements.txt (line 4) because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SKTBrain/KoBERT.git\n",
    "%cd KoBERT\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55941fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6386bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61538cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9febe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09b3f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\PC\\VSC\\bert\\KoBERT\\.cache\\kobert_v1.zip\n",
      "using cached model. C:\\Users\\PC\\VSC\\bert\\KoBERT\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d23b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\".cache/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\".cache/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b876901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\PC\\VSC\\bert\\KoBERT\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f9c6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff68ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e32e5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc316f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae0038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4b045b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4e029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "805237cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "342e2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a31abb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65d9fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b26aeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010963201522827148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2344,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5365b8cb4564edb8907593babe27ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7051867842674255 train acc 0.578125\n",
      "epoch 1 batch id 201 loss 0.5161126852035522 train acc 0.5763370646766169\n",
      "epoch 1 batch id 401 loss 0.4631495773792267 train acc 0.6867206982543641\n",
      "epoch 1 batch id 601 loss 0.4288637638092041 train acc 0.73681884359401\n",
      "epoch 1 batch id 801 loss 0.40208566188812256 train acc 0.7654299313358303\n",
      "epoch 1 batch id 1001 loss 0.32055723667144775 train acc 0.7820772977022977\n",
      "epoch 1 batch id 1201 loss 0.3071320354938507 train acc 0.7947283513738551\n",
      "epoch 1 batch id 1401 loss 0.36454036831855774 train acc 0.8040796752319772\n",
      "epoch 1 batch id 1601 loss 0.32095015048980713 train acc 0.8116118831980013\n",
      "epoch 1 batch id 1801 loss 0.27967509627342224 train acc 0.8179223348139922\n",
      "epoch 1 batch id 2001 loss 0.26608070731163025 train acc 0.8235569715142429\n",
      "epoch 1 batch id 2201 loss 0.2806864380836487 train acc 0.8281675942753294\n",
      "epoch 1 train acc 0.8315246373720137\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008969783782958984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 782,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8528674dee114fcda64fb553f8ff20f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.8827725383631714\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00797271728515625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2344,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc4696f0beb48cfb3a439f0c8f93f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.43884941935539246 train acc 0.8125\n",
      "epoch 2 batch id 201 loss 0.24372044205665588 train acc 0.880752487562189\n",
      "epoch 2 batch id 401 loss 0.24569550156593323 train acc 0.8828709476309227\n",
      "epoch 2 batch id 601 loss 0.3707982301712036 train acc 0.8861012895174709\n",
      "epoch 2 batch id 801 loss 0.41161099076271057 train acc 0.8889864232209738\n",
      "epoch 2 batch id 1001 loss 0.25685617327690125 train acc 0.8913118131868132\n",
      "epoch 2 batch id 1201 loss 0.2036823034286499 train acc 0.8934871981681932\n",
      "epoch 2 batch id 1401 loss 0.21629850566387177 train acc 0.8955545146324054\n",
      "epoch 2 batch id 1601 loss 0.27985796332359314 train acc 0.8975933010618363\n",
      "epoch 2 batch id 1801 loss 0.18193812668323517 train acc 0.8993874930594115\n",
      "epoch 2 batch id 2001 loss 0.22241535782814026 train acc 0.9013305847076462\n",
      "epoch 2 batch id 2201 loss 0.24903827905654907 train acc 0.9026649818264425\n",
      "epoch 2 train acc 0.9041435580204779\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008704900741577148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 782,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62fc10baee84c8182303754469b69d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.8875479539641944\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009162425994873047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2344,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca429b32867458babfc8c0fa650bb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.4762604236602783 train acc 0.78125\n",
      "epoch 3 batch id 201 loss 0.12035330384969711 train acc 0.9224191542288557\n",
      "epoch 3 batch id 401 loss 0.12307340651750565 train acc 0.9232387780548629\n",
      "epoch 3 batch id 601 loss 0.23800422251224518 train acc 0.9247348169717138\n",
      "epoch 3 batch id 801 loss 0.2605189383029938 train acc 0.9281171972534332\n",
      "epoch 3 batch id 1001 loss 0.24368125200271606 train acc 0.9308503996003996\n",
      "epoch 3 batch id 1201 loss 0.10843860357999802 train acc 0.9328164029975021\n",
      "epoch 3 batch id 1401 loss 0.14469993114471436 train acc 0.9348456459671663\n",
      "epoch 3 batch id 1601 loss 0.18491841852664948 train acc 0.9361434259837601\n",
      "epoch 3 batch id 1801 loss 0.09871844947338104 train acc 0.9375086757357024\n",
      "epoch 3 batch id 2001 loss 0.12353959679603577 train acc 0.9389914417791104\n",
      "epoch 3 batch id 2201 loss 0.1502556949853897 train acc 0.9401124488868696\n",
      "epoch 3 train acc 0.9411085039817976\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008970022201538086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 782,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13dad541f1d408ca917d63988369dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.8960597826086957\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009966850280761719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2344,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a421f44642ba4663a81f77e1be3eca1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.45969119668006897 train acc 0.875\n",
      "epoch 4 batch id 201 loss 0.12929657101631165 train acc 0.9556902985074627\n",
      "epoch 4 batch id 401 loss 0.09087707847356796 train acc 0.9560084164588528\n",
      "epoch 4 batch id 601 loss 0.1462012529373169 train acc 0.9576227121464226\n",
      "epoch 4 batch id 801 loss 0.22026221454143524 train acc 0.9592696629213483\n",
      "epoch 4 batch id 1001 loss 0.16651025414466858 train acc 0.9607111638361638\n",
      "epoch 4 batch id 1201 loss 0.05518503859639168 train acc 0.9618156744379683\n",
      "epoch 4 batch id 1401 loss 0.04202696681022644 train acc 0.9627052105638829\n",
      "epoch 4 batch id 1601 loss 0.1369001865386963 train acc 0.9631773110555902\n",
      "epoch 4 batch id 1801 loss 0.07772617787122726 train acc 0.9641258328706275\n",
      "epoch 4 batch id 2001 loss 0.06766048818826675 train acc 0.9650331084457772\n",
      "epoch 4 batch id 2201 loss 0.1497439593076706 train acc 0.9656406179009541\n",
      "epoch 4 train acc 0.9661436113481229\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00996708869934082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 782,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232d4bf25e71454d934052d53faa554a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.8975983056265985\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009966135025024414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2344,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31de93dd978a41c99665fba347f4a474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.4718216359615326 train acc 0.875\n",
      "epoch 5 batch id 201 loss 0.036575496196746826 train acc 0.9738028606965174\n",
      "epoch 5 batch id 401 loss 0.05199874937534332 train acc 0.9753740648379052\n",
      "epoch 5 batch id 601 loss 0.13261984288692474 train acc 0.9757955490848585\n",
      "epoch 5 batch id 801 loss 0.20332427322864532 train acc 0.9762796504369539\n",
      "epoch 5 batch id 1001 loss 0.041516054421663284 train acc 0.977022977022977\n",
      "epoch 5 batch id 1201 loss 0.015168735757470131 train acc 0.977583784346378\n",
      "epoch 5 batch id 1401 loss 0.04586707800626755 train acc 0.9776275874375446\n",
      "epoch 5 batch id 1601 loss 0.040245287120342255 train acc 0.9777873204247346\n",
      "epoch 5 batch id 1801 loss 0.00872697215527296 train acc 0.9780764158800667\n",
      "epoch 5 batch id 2001 loss 0.04306286200881004 train acc 0.9784873188405797\n",
      "epoch 5 batch id 2201 loss 0.10302364826202393 train acc 0.978546683325761\n",
      "epoch 5 train acc 0.9786556100682594\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010963201522827148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 782,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105eb501f53d45d0b4679bd62dbd0f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.8977781329923273\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "557cf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#   tok, max_len, batch_size, device  \n",
    "# comment :     \n",
    "def getSentimentValue(comment, tok, max_len, batch_size, device):\n",
    "  commnetslist = [] #    \n",
    "  emo_list = [] #    \n",
    "  for c in comment: #  \n",
    "    commnetslist.append( [c, 5] ) # [,   ] \n",
    "    \n",
    "  pdData = pd.DataFrame( commnetslist, columns = [['', '']] )\n",
    "  pdData = pdData.values\n",
    "  test_set = BERTDataset(pdData, 0, 1, tok, max_len, True, False) \n",
    "  test_input = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=0)\n",
    "  \n",
    "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_input):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length \n",
    "    # , out   \n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "\t\n",
    "    # e 2    \n",
    "    # 0    ,  \n",
    "    for e in out:\n",
    "      if e[0]>e[1]: # \n",
    "        value = 0\n",
    "      else: #\n",
    "        value = 1\n",
    "      emo_list.append(value)\n",
    "\n",
    "  return emo_list #   11    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "186d300c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = [' ',\n",
    "'','']\n",
    "getSentimentValue(comment, tok, max_len, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "646eb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from pandas) (1.21.6)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\anaconda3\\envs\\bert\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.3.5 pytz-2022.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679d8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
